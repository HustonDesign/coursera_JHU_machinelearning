---
title: "Are your Bicep Curls Correct?"
author: "Diane Reynolds"
date: "January 19, 2016"
output: html_document
---
```{r, echo=FALSE, eval=TRUE, results='hide', error=FALSE, warning=FALSE, comment=FALSE,message=FALSE}
# load libraries
library(e1071)
library(randomForest)
library(dplyr)
library(lattice)
library(rpart)
library(ggplot2)
library(caret)

####################################################
## LOAD THE DATA
####################################################
#load the main data
ndata <- read.csv("pml-training.csv", stringsAsFactor = FALSE,
                  na.strings=c("#DIV/0!","","NA"))
#load test data
vdata <- read.csv("pml-testing.csv", stringsAsFactor = FALSE,
                  na.strings=c("#DIV/0!","","NA"))

# cast the data into the most useful types
ndata$user_name <- as.factor(ndata$user_name)
ndata$classe <- as.factor(ndata$classe)
ndata$new_window <- as.factor(ndata$new_window)
vdata$user_name <- as.factor(vdata$user_name)
vdata$new_window <- as.factor(vdata$new_window)
```

The goal of this analysis is to determine if a bicep curl is being performed correctly, or if incorrectly, which of four mistakes is being made.  The determination is based on observations from four accelerometers positioned on the weight-lifter's body and extrapolated based on data made available by <http://groupware.les.inf.puc-rio.br/har>.

The original analysis, as described in the paper linked above, observed six individuals performing biceps curls correctly and in four incorrect ways.  Many observations were taken at regular points in time with very fine granularity.  

In the original paper, the observations based on data from four accelerometers were summarized using sliding windows of 2.5s.  This created a pool of 406 observations.  Which was analyzed to answer general questions about whether accelometers could give positive and negative feedback to exercisers about their form as well as their rember of repetitions.  Their core result was a confusion matrix showing excellent predictve power using a random forest model.  It is reproduced in Figure 1 below.

####FIGURE 1: Reproduction of Original Paper's Method
```{r, echo=FALSE, eval=TRUE, results='hide', warning=FALSE}
####################################################
## CLEAN THE DATA
####################################################

#filter out non-summary lines / inside of window lines
n1data <- filter(ndata, new_window=="yes")
v1data <- vdata
n1data[is.na(n1data)] <- 0.0
v1data[is.na(v1data)] <- 0.0

#remove columns with little or no variance
nzv <- nearZeroVar(n1data)
n1data <- n1data[, -nzv]
v1data <- v1data[, -nzv]
#remove predictors that are too highly correlated
n1corr = cor(n1data[,8:length(colnames(n1data))-1])
high1corr = findCorrelation(n1corr, cutoff = 0.95) + 7
n1data = n1data[,-high1corr]
v1data = v1data[,-high1corr]
#remove timestamps
n1data = n1data[,-grep("timestamp",colnames(n1data))]
v1data = v1data[,-grep("timestamp",colnames(v1data))]
#remove labels with row numbers, names and window numbers
n1data = n1data[, 2:length(colnames(n1data))]
v1data = v1data[, 2:length(colnames(v1data))]

train1 <- n1data
tovalidate <- v1data

# tidy up, we only need to keep "ndata" and "vdata" going forward
remove(nzv, n1corr, high1corr)

####################################################
## FIT RANDOM FOREST MODELS
####################################################
# fit a random forest model to the Natural data
tex <- train1[,3:length(colnames(train1))]
tex$classe <- as.numeric(tex$classe)
mf1 <- train(classe ~ ., method="rf",data=tex, 
            trControl = trainControl(method="repeatedCV",
                                     number=10,repeats=5))

pmf1 <- predict(mf1,newdata=tex[,1:length(tex)-1])

nttrain1 <- round(pmf1)
nttrain1 <- sapply(nttrain1, function(x) max(1,x))
nttrain1 <- sapply(nttrain1, function(x) min(5,x))
```

```{r, echo=FALSE, eval=TRUE}
confusionMatrix(nttrain1, as.numeric(train1$classe))
```

We then applied this model to the problem at hand: predicting the manner in which the exercise was being performed from a single observation.  We applied the model to the specified 20-observation test set.  The results are shown in Figure 2 below.  Online submission of these results showed a match of only 5/20 results.  Clearly a very different model, analysis or technique will be required to achieve 80% or better power in our case.

####FIGURE 2: Results of the Original Model on Test Set
```{r, echo=FALSE, eval=TRUE}
pmf2 <- predict(mf1,newdata=tovalidate)
bnt <- round(pmf2)
bnt <- sapply(bnt, function(x) max(1,x))
bnt <- sapply(bnt, function(x) min(5,x))
print(bnt)
```

An examination of the test cases themselves showed that in addition to being single-point-in-time observations, all of the participants appeared in the original data set.  This meant there was no need to generalize to the broader population.  Further, there is likely to be significant influence of structural information such as participant and timestamp to locate the proper result in the sequences in the training data.  This increases the need to randomize training and testing subsets carefully.

```{r, echo=FALSE, eval=TRUE, warning=FALSE, results='hide'}
#######################################################
# Clean and filter the data so that the test case info
# is used to its maximum potential when the time comes
#######################################################
#filter out non-summary lines / inside of window lines
ndata <- filter(ndata, new_window=="no")
ndata[is.na(ndata)] <- 0.0
vdata[is.na(vdata)] <- 0.0

#remove columns with little or no variance
vzv <- nearZeroVar(vdata)
ndata <- ndata[, -vzv]
vdata <- vdata[, -vzv]
#remove predictors that are too highly correlated
vcorr = cor(vdata[,8:length(colnames(vdata))-1])
highcorr = findCorrelation(vcorr, cutoff = 0.95) + 7
ndata = ndata[,-highcorr]
vdata = vdata[,-highcorr]
vdata <- vdata[,2:length(colnames(vdata))]
ndata <- ndata[,2:length(colnames(ndata))]

# tidy up, we only need to keep "ndata" and "vdata" going forward
remove(vzv, vcorr, highcorr)

#######################################################
# Separate the data into different training and testing
# sets so that we can make maximum use of it (i.e. prep
# for cross-validation)
#######################################################

set.seed(5170)
samples <- sample(1:length(ndata$classe), length(ndata$classe), replace=FALSE)
# create FIVE sub-samples
marker <- floor(length(ndata$classe)/5)
markers <- c(1, marker, marker*2, marker*3, 
             marker*4, length(ndata$classe))

########################################################
# Use RPart to fit a regression tree
########################################################
# Initially, use only the first set of train and test information
testset <- ndata[samples[markers[1]:markers[2]],]
trainset <- ndata[-samples[markers[1]:markers[2]],]
treeanova <- rpart(classe~., method='anova', data=trainset)
pred <- predict(treeanova,newdata=testset)
accuracy = ( sum(round(pred)==as.numeric(testset$classe))
             / length(testset$classe) ) * 100
```

Because of their flexibility we opt to test a regression tree as an initial model on our re-filtered, now-subdivided data set.  For the initial trial, we choose one of the randomly-generated test/train set pairs and applied a standard regression tree model using rpart with all defaults.  The out-of-sample accuracy, after fitting to the train data and applying that model to the test data was a very good `r accuracy`%.

Our next step was to attempt to recalibrate the regression tree using its parameters, and specifically, the threshold complexity parameter.  Denoted cp, this parameter determines the level of "cut off" in producing the tree.  If the proposed node does not increase the overall R-square of the model by at least cp then the node is not added and the branch ceases.  This controls the tree size.

```{r, echo=FALSE, eval=TRUE, warning=FALSE, results='hide'}
########################################################
# Use RPart with cp=0.001 to fit a regression tree
########################################################
# Initially, use only the first set of train and test information
testset <- ndata[samples[markers[1]:markers[2]],]
trainset <- ndata[-samples[markers[1]:markers[2]],]
treeanova <- rpart(classe~., method='anova', data=trainset, cp=0.001)
pred <- predict(treeanova,newdata=testset)
accuracy = ( sum(round(pred)==as.numeric(testset$classe))
             / length(testset$classe) ) * 100
```

Thus, one way to increase the accuracy of the model might be to decrease the threshold.  Rather than maintaining the default value of 0.01, we reset the parameter to 0.001.  This produced a much higer accuracy of `r accuracy`% in the out-of-sample test.  The question now becomes whether or not such a high rate of accuracy is typical, or whether it was just luck.

Accordingly, we compute the out-of-sample accuracy for each element in the cross-validation.  This means fitting the model five times and applying each fitted model to the associated, independent test set.  The results of the five runs are shown in Figure 3, below.

####FIGURE 3: Out-of-Sample Accuracy in Cross-Validation
```{r, echo=FALSE, eval=TRUE, warning=FALSE}
########################################################
# Verify out-of-sample estimates with cross-validation
########################################################

for (i in 1:5)
{
  testset <- ndata[samples[markers[i]:markers[i+1]],]
  trainset <- ndata[-samples[markers[i]:markers[i+1]],]
  
  treeanova <- rpart(classe~., method='anova', 
                     data=trainset, cp=0.001)
  pred <- predict(treeanova,newdata=testset)
  accuracy = ( sum(round(pred)==as.numeric(testset$classe))
                   / length(testset$classe) ) * 100
  print(accuracy)
}
```

With this powerful result, we comfortably estimate the out-of-sample error to be approximately 95.7%. This was done by taking the mean of the five out-of-sample accuracies above and rounding.

We then used all available training data to fit a regression tree, retaining the parameter of cp=0.001.  Running the sample test set of 20 observations produces the results shown in Figure 3 below. Translating the numerical codes to the first five letters of the alphabet and submitting them on line, our quiz result was 20/20.  This is generally well-aligned to our estimated out-of-sample error.

####FIGURE 4: Quiz Responses from Best-Fit Model
```{r, echo=FALSE, eval=TRUE, warning=FALSE}
########################################################
# Produce quiz answers
########################################################

treeanova <- rpart(classe~., method='anova', data=ndata, cp=0.001)
pred <- predict(treeanova,newdata=vdata)
print(round(pred))
```

##Conclusion
In conclusion, we found that a regression tree fit using a threshold of 0.001 produced a very good predictive model both in- and out-of-sample.  It was sufficient to estimate the test samples accurately in all 20 cases.  However, in another sense, this model is somewhat lacking.  It used "internal" factors to the experiment in its tree, including time stamps.  If this model were to be applied to other participants or samples from different days or times, it is unlikely that it would perform well at all.  Thus, our 95.7% out-of-sample accuracy prediction is limited to the strict definition of out-of-sample imposed by this assignment.